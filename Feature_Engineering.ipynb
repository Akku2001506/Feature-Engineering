{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Feature Engineering\n"
      ],
      "metadata": {
        "id": "R3Iugsz92CyQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is a parameter?\n",
        "- A **parameter** is a variable used to pass information into a function, method, or procedure.\n",
        "\n",
        "### In simple terms:\n",
        "A parameter is like a placeholder in a function that tells the function what kind of input it can expect.\n",
        "\n",
        "### Example (in Python):\n",
        "```python\n",
        "def greet(name):\n",
        "    print(f\"Hello, {name}!\")\n",
        "```\n",
        "\n",
        "Here, `name` is a **parameter** of the `greet` function. When you call the function with a value:\n",
        "```python\n",
        "greet(\"Alice\")\n",
        "```\n",
        "\n",
        "The value `\"Alice\"` is called an **argument**, and it gets passed into the parameter `name`.\n",
        "\n",
        "### In summary:\n",
        "- **Parameter**: The variable in the function definition (e.g., `name`)\n",
        "- **Argument**: The actual value you pass when calling the function (e.g., `\"Alice\"`)\n",
        "\n",
        "----\n",
        "2. What is correlation?\n",
        "- **Correlation** is a statistical measure that describes how strongly two variables are related to each other.\n",
        "\n",
        "### In simple terms:\n",
        "It tells you whether, and how, changes in one variable are associated with changes in another.\n",
        "\n",
        "---\n",
        "\n",
        "### Types of Correlation:\n",
        "1. **Positive correlation**: As one variable increases, the other also increases.  \n",
        "   Example: Height and weight‚Äîtaller people often weigh more.\n",
        "\n",
        "2. **Negative correlation**: As one variable increases, the other decreases.  \n",
        "   Example: The more you exercise, the less you might weigh.\n",
        "\n",
        "3. **No correlation**: The variables don't seem to affect each other.  \n",
        "   Example: Shoe size and intelligence.\n",
        "\n",
        "---\n",
        "\n",
        "### Correlation Coefficient (usually **r**):\n",
        "- Ranges from **-1 to 1**\n",
        "  - `r = 1`: Perfect positive correlation\n",
        "  - `r = -1`: Perfect negative correlation\n",
        "  - `r = 0`: No correlation\n",
        "\n",
        "---\n",
        "\n",
        "### Visual Example:\n",
        "- If you plotted data on a graph:\n",
        "  - A line going **upward** = positive correlation\n",
        "  - A line going **downward** = negative correlation\n",
        "  - A **scatter** with no pattern = no correlation\n",
        "\n",
        "---\n",
        "What does negative correlation mean?\n",
        "- **Negative correlation** means that as one variable increases, the other **decreases**‚Äîthey move in opposite directions.\n",
        "\n",
        "---\n",
        "\n",
        "###  In simple terms:\n",
        "When **X goes up**, **Y goes down** ‚Äî and vice versa.\n",
        "\n",
        "---\n",
        "\n",
        "### üìä Example:\n",
        "- The more time you spend on social media (X), the lower your productivity might be (Y).  \n",
        "  ‚Üí As screen time increases, productivity decreases.\n",
        "\n",
        "---\n",
        "\n",
        "### Real-world examples:\n",
        "- Number of missed classes and exam scores  \n",
        "  (More missed classes ‚Üí Lower scores)\n",
        "- Speed of a car and time taken to reach a destination  \n",
        "  (Higher speed ‚Üí Less time)\n",
        "\n",
        "---\n",
        "\n",
        "### üìê In numbers (correlation coefficient **r**):\n",
        "- `r = -1`: Perfect negative correlation  \n",
        "- `r = -0.5`: Moderate negative correlation  \n",
        "- `r = 0`: No correlation\n",
        "\n",
        "---\n",
        "3. Define Machine Learning. What are the main components in Machine Learning?\n",
        "\n",
        "- ### ‚úÖ **Definition of Machine Learning (ML):**\n",
        "**Machine Learning** is a branch of artificial intelligence (AI) that enables systems to **learn from data** and **make decisions or predictions** without being explicitly programmed.\n",
        "\n",
        "In simple terms:  \n",
        "> It‚Äôs teaching computers to learn patterns from data so they can make predictions or decisions on their own.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚öôÔ∏è **Main Components of Machine Learning:**\n",
        "\n",
        "1. **Data**  \n",
        "   - The foundation. ML learns from examples.\n",
        "   - Can be structured (tables), unstructured (images, text), or semi-structured.\n",
        "\n",
        "2. **Features (Input Variables)**  \n",
        "   - Measurable properties or characteristics used for learning.\n",
        "   - Example: For house price prediction, features might be size, location, number of rooms.\n",
        "\n",
        "3. **Model (Algorithm)**  \n",
        "   - The logic or math used to find patterns in the data.\n",
        "   - Examples: Linear Regression, Decision Trees, Neural Networks.\n",
        "\n",
        "4. **Training**  \n",
        "   - Feeding data to the model so it can learn patterns.\n",
        "   - The model adjusts itself to minimize errors during this phase.\n",
        "\n",
        "5. **Labels (for Supervised Learning)**  \n",
        "   - The correct answers used to train the model.\n",
        "   - Example: Images labeled as ‚Äúcat‚Äù or ‚Äúdog‚Äù so the model learns to classify.\n",
        "\n",
        "6. **Loss Function**  \n",
        "   - A way to measure how well (or badly) the model is performing.\n",
        "   - Lower loss = better predictions.\n",
        "\n",
        "7. **Optimization Algorithm**  \n",
        "   - Tweaks the model to improve performance (usually during training).\n",
        "   - Example: Gradient Descent.\n",
        "\n",
        "8. **Testing/Validation**  \n",
        "   - Checks how well the model performs on new, unseen data.\n",
        "\n",
        "9. **Prediction**  \n",
        "   - Using the trained model to make decisions or forecasts.\n",
        "\n",
        "---\n",
        "\n",
        "4.  How does loss value help in determining whether the model is good or not?\n",
        "- Great question!\n",
        "\n",
        "###  What is **loss value**?\n",
        "The **loss value** is a number that tells us **how far off the model‚Äôs predictions are from the actual values**.  \n",
        "It‚Äôs basically the model‚Äôs **\"error score.\"**\n",
        "\n",
        "---\n",
        "\n",
        "###  How it helps determine if the model is good:\n",
        "\n",
        "-  **Low loss** = The model is making predictions close to the actual values ‚Üí **Good performance**\n",
        "-  **High loss** = The model‚Äôs predictions are way off ‚Üí **Poor performance**\n",
        "\n",
        "---\n",
        "\n",
        "###  During training:\n",
        "- The model **tries to minimize the loss** by adjusting its internal parameters.\n",
        "- The goal is to get the **lowest possible loss** on training data **and** unseen data (validation/test).\n",
        "\n",
        "---\n",
        "\n",
        "###  Example:\n",
        "Let‚Äôs say the actual price of a house is **$300,000**.\n",
        "\n",
        "- Your model predicts **$295,000** ‚Üí small error ‚Üí **low loss**\n",
        "- Your model predicts **$100,000** ‚Üí large error ‚Üí **high loss**\n",
        "\n",
        "The loss value quantifies this error.\n",
        "\n",
        "---\n",
        "\n",
        "### Bonus: Visualizing the loss\n",
        "If you plot the loss value over time (during training), a **decreasing curve** is a good sign.  \n",
        "If it's **stuck** or **increasing**, the model might be struggling.\n",
        "\n",
        "---\n",
        "5. What are continuous and categorical variables?\n",
        "- Great question! Let's break it down nice and simple:\n",
        "\n",
        "---\n",
        "\n",
        "### üìä **Continuous Variables:**\n",
        "\n",
        "- These are **numerical** variables that can take **any value within a range**.\n",
        "- You can **measure** them.\n",
        "- They can have **decimals** or **fractions**.\n",
        "\n",
        "#### ‚úÖ Examples:\n",
        "- Height (e.g., 5.8 feet)\n",
        "- Temperature (e.g., 98.6¬∞F)\n",
        "- Weight (e.g., 72.5 kg)\n",
        "- Price (e.g., $9.99)\n",
        "\n",
        "#### üéØ Think: **‚ÄúHow much?‚Äù**\n",
        "\n",
        "---\n",
        "\n",
        "### üß± **Categorical Variables:**\n",
        "\n",
        "- These are variables that represent **categories or groups**.\n",
        "- They usually contain **labels** or **names**, not numbers used for math.\n",
        "- Can be **nominal** (no order) or **ordinal** (with order).\n",
        "\n",
        "#### ‚úÖ Examples:\n",
        "- Gender (Male, Female, Other)\n",
        "- Colors (Red, Blue, Green)\n",
        "- Education level (High School, Bachelor's, Master's)\n",
        "- Yes/No answers\n",
        "\n",
        "#### üéØ Think: **‚ÄúWhat type?‚Äù or ‚ÄúWhich category?‚Äù**\n",
        "\n",
        "---\n",
        "\n",
        "### Quick Comparison Table:\n",
        "\n",
        "| Feature              | Continuous             | Categorical               |\n",
        "|----------------------|------------------------|---------------------------|\n",
        "| Type                 | Numeric                | Labels or categories      |\n",
        "| Range                | Infinite (within limits)| Limited, predefined       |\n",
        "| Examples             | Age, income, speed     | Country, gender, color    |\n",
        "| Can do math on it?   | Yes                    | No (not directly)         |\n",
        "\n",
        "---\n",
        "\n",
        "6. How do we handle categorical variables in Machine Learning? What are the common t\n",
        " echniques?\n",
        "- Great question! Since most Machine Learning models work best with **numbers**, we need to **convert categorical variables** into a numerical format that models can understand.\n",
        "\n",
        "---\n",
        "\n",
        "### üîß **Common Techniques to Handle Categorical Variables:**\n",
        "\n",
        "---\n",
        "\n",
        "### 1. **Label Encoding**\n",
        "- Converts each category into a unique number.\n",
        "  \n",
        "  Example:\n",
        "  ```\n",
        "  Color: [Red, Green, Blue] ‚Üí [0, 1, 2]\n",
        "  ```\n",
        "\n",
        "- ‚úÖ Pros: Simple, fast  \n",
        "- ‚ùå Cons: Implies order (which might not be true)\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **One-Hot Encoding**\n",
        "- Creates a new binary column for each category.\n",
        "  \n",
        "  Example:\n",
        "  ```\n",
        "  Color: Red ‚Üí [1, 0, 0]\n",
        "         Green ‚Üí [0, 1, 0]\n",
        "         Blue ‚Üí [0, 0, 1]\n",
        "  ```\n",
        "\n",
        "- ‚úÖ Pros: No false sense of order  \n",
        "- ‚ùå Cons: Can create a *lot* of columns if there are many categories (\"curse of dimensionality\")\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Ordinal Encoding**\n",
        "- Use when categories have a **natural order** (e.g., \"low\", \"medium\", \"high\").\n",
        "  \n",
        "  Example:\n",
        "  ```\n",
        "  Size: [Small, Medium, Large] ‚Üí [0, 1, 2]\n",
        "  ```\n",
        "\n",
        "- ‚úÖ Pros: Keeps meaningful order  \n",
        "- ‚ùå Cons: Should only be used when the order matters\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **Target Encoding (Mean Encoding)**\n",
        "- Replace categories with the **average value of the target** for each category.\n",
        "  \n",
        "  Example:\n",
        "  If you‚Äôre predicting sales, and:\n",
        "  ```\n",
        "  Category A ‚Üí avg sales = 100\n",
        "  Category B ‚Üí avg sales = 200\n",
        "  ```\n",
        "\n",
        "- ‚úÖ Pros: Can be powerful  \n",
        "- ‚ùå Cons: Prone to overfitting (best with cross-validation or smoothing)\n",
        "\n",
        "---\n",
        "\n",
        "### 5. **Binary Encoding / Hash Encoding**  \n",
        "- Advanced techniques used for high-cardinality features (many categories).\n",
        "- More compact than One-Hot, less prone to overfitting than Target Encoding.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚ö†Ô∏è **Important Notes:**\n",
        "- Choose the method based on:\n",
        "  - Type of model (tree-based models handle labels better)\n",
        "  - Number of categories\n",
        "  - Whether the category has a logical order\n",
        "- Always handle **missing values** before encoding.\n",
        "\n",
        "---\n",
        "\n",
        "7. What do you mean by training and testing a dataset?\n",
        "- Awesome question ‚Äî this is a key concept in Machine Learning!\n",
        "\n",
        "---\n",
        "\n",
        "### üß† What is **Training and Testing a Dataset**?\n",
        "\n",
        "When building a Machine Learning model, you want it to **learn** from some data (training), and then **see how well it performs** on new, unseen data (testing). That‚Äôs where the dataset is **split** into two (or more) parts:\n",
        "\n",
        "---\n",
        "\n",
        "### üìò 1. **Training Set**\n",
        "- The part of the data used to **train the model**.\n",
        "- The model **learns patterns** from this data.\n",
        "- Think of it as \"studying\" or \"practice time\" for the model.\n",
        "\n",
        "---\n",
        "\n",
        "### üß™ 2. **Testing Set**\n",
        "- The part of the data used to **evaluate the model‚Äôs performance**.\n",
        "- The model has **never seen this data before**.\n",
        "- Think of it as the \"exam\" after studying.\n",
        "\n",
        "---\n",
        "\n",
        "### üìä Why split the data?\n",
        "To make sure the model:\n",
        "- Doesn‚Äôt just **memorize** the training data\n",
        "- Can actually **generalize** to new data it hasn't seen before\n",
        "\n",
        "---\n",
        "\n",
        "### üß© Common Split Ratios:\n",
        "- **80% training / 20% testing**\n",
        "- **70% training / 30% testing**\n",
        "- Sometimes also a **validation set** is used (for tuning the model before final testing)\n",
        "\n",
        "---\n",
        "\n",
        "### üí° Analogy:\n",
        "> Training set = practice problems  \n",
        "> Testing set = final exam  \n",
        "> Validation set (if used) = mock test\n",
        "\n",
        "---\n",
        "\n",
        "8. What is sklearn.preprocessing?\n",
        "- Great question! Let's break it down:\n",
        "\n",
        "---\n",
        "\n",
        "### üß™ **`sklearn.preprocessing`** (from Scikit-learn)\n",
        "\n",
        "It‚Äôs a **module** in the `scikit-learn` library used for **preprocessing data** ‚Äî basically **cleaning, scaling, transforming, or encoding** your features before training a Machine Learning model.\n",
        "\n",
        "---\n",
        "\n",
        "### üîß Why use it?\n",
        "Most ML models don‚Äôt work well unless your data is:\n",
        "- Scaled properly\n",
        "- Encoded numerically\n",
        "- Cleaned up\n",
        "\n",
        "So `sklearn.preprocessing` helps **get your data model-ready**.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚öôÔ∏è Common Tools in `sklearn.preprocessing`:\n",
        "\n",
        "| Tool | What It Does |\n",
        "|------|---------------|\n",
        "| **`StandardScaler`** | Scales data to have **mean = 0**, **std = 1** (Z-score normalization) |\n",
        "| **`MinMaxScaler`** | Scales data to a **range**, typically [0, 1] |\n",
        "| **`LabelEncoder`** | Converts categorical **labels** into numbers |\n",
        "| **`OneHotEncoder`** | Converts categorical features into **binary columns** |\n",
        "| **`Binarizer`** | Converts numeric values to 0 or 1 based on a threshold |\n",
        "| **`PolynomialFeatures`** | Generates new features (e.g., x¬≤, x¬≥) for polynomial regression |\n",
        "| **`Normalizer`** | Scales each **individual sample** (row) to unit norm |\n",
        "| **`FunctionTransformer`** | Lets you apply **custom transformations** to data |\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Example: Using `StandardScaler`\n",
        "```python\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "data = np.array([[1, 2], [3, 4], [5, 6]])\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "print(scaled_data)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "9. What is a Test set?\n",
        "- Great question ‚Äî this is a core concept in evaluating how good a machine learning model really is.\n",
        "\n",
        "---\n",
        "\n",
        "### üß™ **What is a Test Set?**\n",
        "\n",
        "A **test set** is a portion of your dataset that is **not used during training**, but is used to **evaluate the final performance** of your trained model.\n",
        "\n",
        "---\n",
        "\n",
        "### üìò In simple terms:\n",
        "\n",
        "- **Training set** = What the model **learns from**\n",
        "- **Test set** = What the model is **tested on** to see how well it learned\n",
        "\n",
        "---\n",
        "\n",
        "### üîç Why is it important?\n",
        "\n",
        "- It simulates **real-world, unseen data**.\n",
        "- Helps you check if your model can **generalize** beyond the training data.\n",
        "- Prevents **overfitting** (where a model performs great on training data but fails on new data).\n",
        "\n",
        "---\n",
        "\n",
        "### üìä Example:\n",
        "\n",
        "Suppose you have 1,000 rows of data.\n",
        "\n",
        "- You might use **800 rows** for training (80%)\n",
        "- And **200 rows** for testing (20%)\n",
        "\n",
        "After training your model on the 800 rows, you test it on the 200 rows to measure accuracy, precision, recall, etc.\n",
        "\n",
        "---\n",
        "\n",
        "10. How do we split data for model fitting (training and testing) in Python?\n",
        "- Great! In Python, the most common way to split your data into training and testing sets is using **`train_test_split`** from the **`sklearn.model_selection`** module.\n",
        "\n",
        "---\n",
        "\n",
        "### üîß **Step-by-step: Splitting Data with `train_test_split`**\n",
        "\n",
        "Here‚Äôs a basic example:\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Example data\n",
        "X = [[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]]  # Features\n",
        "y = [0, 1, 0, 1, 0, 1, 0, 1, 0, 1]                      # Labels\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,                # Features and labels\n",
        "    test_size=0.2,       # 20% for testing\n",
        "    random_state=42      # For reproducibility (optional)\n",
        ")\n",
        "\n",
        "print(\"Training set:\", X_train)\n",
        "print(\"Testing set:\", X_test)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üß© Parameters of `train_test_split`:\n",
        "| Parameter      | Description |\n",
        "|----------------|-------------|\n",
        "| `test_size`    | Fraction (e.g., `0.2` for 20%) or number of samples for the test set |\n",
        "| `train_size`   | Optional ‚Äì you can specify this instead of `test_size` |\n",
        "| `random_state` | Ensures the same split every time you run it |\n",
        "| `shuffle`      | Whether to shuffle before splitting (default is `True`) |\n",
        "| `stratify`     | Used to maintain class balance, especially in classification tasks |\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Tip:\n",
        "If you're working with labeled classification data (like 0s and 1s), use `stratify=y` to keep class distribution the same in both train and test sets:\n",
        "```python\n",
        "train_test_split(X, y, test_size=0.2, stratify=y)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        " How do you approach a Machine Learning problem?\n",
        " - Awesome question ‚Äî this is where the real magic happens ‚ú®\n",
        "\n",
        "---\n",
        "\n",
        "### üß† **How to Approach a Machine Learning Problem (Step-by-Step):**\n",
        "\n",
        "Think of it like a structured **game plan**. Here‚Äôs a tried-and-true process:\n",
        "\n",
        "---\n",
        "\n",
        "### 1. **Understand the Problem**\n",
        "- What's the goal? Classification? Regression? Clustering?\n",
        "- What are you trying to predict or discover?\n",
        "- Know the **business context** or real-world impact.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Collect the Data**\n",
        "- Gather the dataset(s).\n",
        "- Can be from CSVs, APIs, databases, scraping, etc.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Explore the Data (EDA - Exploratory Data Analysis)**\n",
        "- Look at the shape, types, missing values.\n",
        "- Use visualizations (histograms, boxplots, scatter plots).\n",
        "- Identify potential outliers, correlations, and patterns.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **Preprocess the Data**\n",
        "- **Clean**: Handle missing data, duplicates, outliers.\n",
        "- **Encode**: Convert categorical variables (Label/One-Hot Encoding).\n",
        "- **Scale**: Normalize or standardize numerical values.\n",
        "- **Split**: Divide into training and testing sets (and maybe validation).\n",
        "\n",
        "---\n",
        "\n",
        "### 5. **Select a Model**\n",
        "- Choose a model based on the problem type:\n",
        "  - Classification ‚Üí Logistic Regression, Decision Tree, SVM, etc.\n",
        "  - Regression ‚Üí Linear Regression, Random Forest Regressor, etc.\n",
        "  - Clustering ‚Üí KMeans, DBSCAN\n",
        "  - Deep learning ‚Üí CNNs, RNNs (for image/text)\n",
        "\n",
        "---\n",
        "\n",
        "### 6. **Train the Model**\n",
        "- Feed the training data to the model.\n",
        "- The model learns the patterns.\n",
        "\n",
        "---\n",
        "\n",
        "### 7. **Evaluate the Model**\n",
        "- Use the **test set** to measure performance.\n",
        "- Metrics:\n",
        "  - Classification: Accuracy, Precision, Recall, F1-Score\n",
        "  - Regression: MSE, RMSE, MAE, R¬≤\n",
        "\n",
        "---\n",
        "\n",
        "### 8. **Tune the Model (Optional but powerful)**\n",
        "- Improve performance using:\n",
        "  - Hyperparameter tuning (Grid Search, Random Search)\n",
        "  - Cross-validation\n",
        "  - Feature selection/engineering\n",
        "\n",
        "---\n",
        "\n",
        "### 9. **Deploy the Model**\n",
        "- Use it in a real-world application: web app, API, automation pipeline.\n",
        "\n",
        "---\n",
        "\n",
        "### 10. **Monitor and Maintain**\n",
        "- Track performance over time.\n",
        "- Retrain with new data if needed.\n",
        "\n",
        "---\n",
        "\n",
        "11. Why do we have to perform EDA before fitting a model to the data?\n",
        "- Fantastic question ‚Äî and one that **every good data scientist** should ask before jumping into model training.\n",
        "\n",
        "---\n",
        "\n",
        "### üîç Why Perform **EDA** (Exploratory Data Analysis) Before Fitting a Model?\n",
        "\n",
        "EDA is like **getting to know your data before trusting it**. You wouldn‚Äôt cook a meal without checking your ingredients, right?\n",
        "\n",
        "Here‚Äôs why EDA is essential:\n",
        "\n",
        "---\n",
        "\n",
        "### 1. **Understand the Data's Structure**\n",
        "- How many rows & columns?\n",
        "- What are the types of each variable (numeric, categorical, etc.)?\n",
        "- Are there missing values? Duplicates?\n",
        "\n",
        "‚û°Ô∏è This helps you plan preprocessing steps and spot problems early.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Detect Data Quality Issues**\n",
        "- **Missing data**, **outliers**, or **inconsistent formats** can mess up model training.\n",
        "- Example: An age of 500 or a salary of -$1000 should raise red flags üö©\n",
        "\n",
        "‚û°Ô∏è Fixing these early saves tons of headaches later.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Reveal Patterns and Relationships**\n",
        "- Which features are correlated with the target?\n",
        "- Do you notice trends, clusters, or class imbalances?\n",
        "\n",
        "‚û°Ô∏è This helps with **feature selection** and **model choice**.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **Guide Feature Engineering**\n",
        "- You might notice that a feature needs to be split, binned, or combined.\n",
        "- For example, breaking a ‚Äúdate‚Äù into ‚Äúday,‚Äù ‚Äúmonth,‚Äù and ‚Äúyear.‚Äù\n",
        "\n",
        "‚û°Ô∏è This improves the signal your model can learn from.\n",
        "\n",
        "---\n",
        "\n",
        "### 5. **Choose the Right Model or Metric**\n",
        "- EDA might reveal class imbalance ‚Üí maybe accuracy isn't the best metric.\n",
        "- Or that your target variable isn‚Äôt linear ‚Üí maybe linear regression isn‚Äôt ideal.\n",
        "\n",
        "‚û°Ô∏è Helps you make smart model decisions.\n",
        "\n",
        "---\n",
        "\n",
        "### 6. **Avoid Garbage-In, Garbage-Out**\n",
        "If you skip EDA, your model could be learning from **noisy, biased, or meaningless data**.  \n",
        "‚û°Ô∏è Result: Bad predictions, even if the model seems fine during training.\n",
        "\n",
        "---\n",
        "\n",
        "### üí° Quick EDA Techniques:\n",
        "- `df.info()`, `df.describe()`\n",
        "- Histograms, box plots, scatter plots\n",
        "- Correlation matrix\n",
        "- Missing value heatmaps\n",
        "- Value counts for categorical features\n",
        "\n",
        "---\n",
        "\n",
        "### Summary:\n",
        "> **EDA is not optional** ‚Äî it‚Äôs like reading the map before starting the journey.  \n",
        "> It gives you **insight**, **direction**, and **trust** in your data before you hand it to your model.\n",
        "\n",
        "---\n",
        "\n",
        "12. What is correlation?\n",
        "\n",
        "---\n",
        "\n",
        "### üîó **What is Correlation?**\n",
        "\n",
        "**Correlation** is a **statistical measure** that tells us how **two variables are related** to each other.\n",
        "\n",
        "It answers the question:  \n",
        "> *‚ÄúWhen one variable changes, does the other change too? And if so, how?‚Äù*\n",
        "\n",
        "---\n",
        "\n",
        "### üìä **Types of Correlation:**\n",
        "\n",
        "| Type               | Description |\n",
        "|--------------------|-------------|\n",
        "| **Positive**       | Both variables increase or decrease together üìàüìà (e.g., height and weight) |\n",
        "| **Negative**       | One increases while the other decreases üìàüìâ (e.g., speed and travel time) |\n",
        "| **Zero / No Correlation** | No relationship between variables ‚ùå (e.g., shoe size and intelligence) |\n",
        "\n",
        "---\n",
        "\n",
        "### üî¢ **Correlation Coefficient (r)**\n",
        "\n",
        "This value tells you **how strong and what direction** the correlation is.  \n",
        "It ranges from **-1 to +1**:\n",
        "\n",
        "| r-value | Interpretation           |\n",
        "|---------|---------------------------|\n",
        "| +1      | Perfect positive correlation |\n",
        "| 0       | No correlation               |\n",
        "| -1      | Perfect negative correlation |\n",
        "\n",
        "‚úÖ Closer to ¬±1 = stronger relationship  \n",
        "‚ùå Closer to 0 = weaker relationship\n",
        "\n",
        "---\n",
        "\n",
        "### üìâ Example:\n",
        "\n",
        "Imagine this:\n",
        "\n",
        "| Study Time (hrs) | Exam Score (%) |\n",
        "|------------------|----------------|\n",
        "| 1                | 50             |\n",
        "| 2                | 60             |\n",
        "| 3                | 70             |\n",
        "\n",
        "This shows **positive correlation** ‚Äî as study time increases, exam scores increase.\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Why is it useful in ML?\n",
        "- Helps you **choose the most useful features** for your model.\n",
        "- Can **reveal multicollinearity** (features that are too similar ‚Äî not always good).\n",
        "\n",
        "---\n",
        "13. What does negative correlation mean?\n",
        "\n",
        "---\n",
        "\n",
        "### üîÅ **What Does Negative Correlation Mean?**\n",
        "\n",
        "**Negative correlation** means that **as one variable increases, the other decreases** ‚Äî they move in **opposite directions**.\n",
        "\n",
        "---\n",
        "\n",
        "### üìâ Real-Life Examples:\n",
        "\n",
        "| Variable A               | Variable B                  | What Happens              |\n",
        "|--------------------------|-----------------------------|---------------------------|\n",
        "| Outside temperature ‚Üë    | Heater usage ‚Üì              | Warmer ‚Üí less heater used |\n",
        "| Speed of a car ‚Üë         | Travel time ‚Üì               | Faster ‚Üí shorter time     |\n",
        "| Exercise time ‚Üë          | Body fat % ‚Üì                | More workouts ‚Üí less fat  |\n",
        "\n",
        "---\n",
        "\n",
        "### üî¢ The Math Side:\n",
        "\n",
        "Negative correlation has a **correlation coefficient (r)** between **0 and -1**:\n",
        "- **-1** = perfect negative correlation  \n",
        "- **-0.5** = moderate negative correlation  \n",
        "- **0** = no correlation\n",
        "\n",
        "So the more **strongly negative** the number, the stronger the opposite relationship.\n",
        "\n",
        "---\n",
        "\n",
        "### üìä Visualization:\n",
        "\n",
        "Imagine a scatter plot where the dots **go down from left to right** ‚Äî that‚Äôs a negative slope, and it shows negative correlation.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Summary:\n",
        "\n",
        "> **Negative correlation = One goes up, the other goes down.**\n",
        "\n",
        "It's super useful in ML to identify features that may have **inverse relationships** with your target variable.\n",
        "\n",
        "---\n",
        "\n",
        "14.  How can you find correlation between variables in Python?\n",
        "- Great question! Finding correlation in Python is super easy and powerful using libraries like **`pandas`** and **`seaborn`**.\n",
        "\n",
        "Here‚Äôs how you can do it step-by-step üëá\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **1. Using Pandas `.corr()` Method**\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "# Sample data\n",
        "data = {\n",
        "    'Hours_Studied': [1, 2, 3, 4, 5],\n",
        "    'Exam_Score': [55, 60, 65, 70, 75],\n",
        "    'TV_Watched': [10, 8, 6, 4, 2]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Calculate correlation matrix\n",
        "correlation_matrix = df.corr()\n",
        "print(correlation_matrix)\n",
        "```\n",
        "\n",
        "This will give you the **correlation coefficients** between all numeric variables.\n",
        "\n",
        "---\n",
        "\n",
        "### üîç **2. Visualizing with Seaborn (Heatmap)**\n",
        "\n",
        "```python\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
        "plt.title(\"Correlation Heatmap\")\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "- `annot=True` adds the correlation values inside the boxes.\n",
        "- `cmap='coolwarm'` shows positive correlation in red and negative in blue.\n",
        "\n",
        "---\n",
        "\n",
        "### üìå Notes:\n",
        "- Only works on **numerical columns**.\n",
        "- Correlation type by default is **Pearson**. You can also use:\n",
        "  - `.corr(method='spearman')`\n",
        "  - `.corr(method='kendall')`\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Example Insight:\n",
        "If you see `Exam_Score` and `TV_Watched` have **negative correlation**, it might suggest:\n",
        "> \"More TV = Lower scores\" üìâ\n",
        "\n",
        "---\n",
        "\n",
        "15. What is causation? Explain difference between correlation and causation with an example.\n",
        "- Great question! üôå A lot of people mix up **correlation** and **causation**, but they mean very different things ‚Äî especially in data science and statistics.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚ö°Ô∏è **What is Causation?**\n",
        "\n",
        "**Causation** means **one variable *directly causes* a change in another**.\n",
        "\n",
        "> If **A causes B**, then changing A will make B change.\n",
        "\n",
        "It's about **cause and effect**.\n",
        "\n",
        "---\n",
        "\n",
        "### üîÑ **Correlation vs. Causation**\n",
        "\n",
        "| Feature         | Correlation                           | Causation                              |\n",
        "|----------------|----------------------------------------|----------------------------------------|\n",
        "| üîó Relationship | Two variables change together          | One variable causes the other to change |\n",
        "| ‚ùì Why?         | Might be due to chance, or a third variable | There is a direct cause-effect link     |\n",
        "| ‚úÖ Proof Needed | No ‚Äî just patterns                     | Yes ‚Äî needs experiments or deep study   |\n",
        "\n",
        "---\n",
        "\n",
        "### ü§Ø **Example: Ice Cream & Drowning**\n",
        "\n",
        "- **Observation:** Ice cream sales ‚Üë when drowning cases ‚Üë  \n",
        "- **Correlation?** Yes!  \n",
        "- **Causation?** Nope.\n",
        "\n",
        "> üç¶ Ice cream doesn‚Äôt cause drowning.  \n",
        "> ‚òÄÔ∏è **The real cause?** Hot weather. People swim more (and eat more ice cream).\n",
        "\n",
        "That‚Äôs a **spurious correlation** ‚Äî looks related, but it‚Äôs not causal.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **Example of Causation**\n",
        "\n",
        "- **A**: Taking medicine  \n",
        "- **B**: Getting better\n",
        "\n",
        "If studies prove that the medicine helps recovery, then we can say:\n",
        "> Taking the medicine **causes** improvement.\n",
        "\n",
        "---\n",
        "\n",
        "### üö® Why it matters in Machine Learning:\n",
        "Just because a feature is correlated with the target, **doesn't mean it influences it**.  \n",
        "‚Üí So be careful when selecting features or drawing conclusions.\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Summary:\n",
        "\n",
        "> **Correlation ‚â† Causation**  \n",
        "> Correlation shows a pattern.  \n",
        "> Causation proves a **reason**.\n",
        "\n",
        "---\n",
        "\n",
        "16. What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
        "- Awesome question! üôå Optimizers are a **core part** of how Machine Learning ‚Äî especially **Deep Learning** ‚Äî actually works. Let‚Äôs break it down clearly üëá\n",
        "\n",
        "---\n",
        "\n",
        "### ‚öôÔ∏è **What is an Optimizer?**\n",
        "\n",
        "An **optimizer** is an algorithm that **adjusts the model‚Äôs parameters (like weights and biases)** during training to **minimize the loss function**.\n",
        "\n",
        "> Think of it like a **GPS** trying to find the **shortest path** to the best solution ‚Äî a.k.a. the point where the model makes the least mistakes.\n",
        "\n",
        "---\n",
        "\n",
        "### üîÅ What Does It Do?\n",
        "- Takes the **loss value**\n",
        "- Computes **gradients** using **backpropagation**\n",
        "- Updates the model's parameters to reduce the loss\n",
        "- Repeats this over many iterations (epochs)\n",
        "\n",
        "---\n",
        "\n",
        "### üöÄ **Common Optimizers (in Deep Learning)**\n",
        "\n",
        "#### 1. **Gradient Descent (GD)**\n",
        "- The most basic optimizer\n",
        "- Adjusts weights in the **opposite direction of the gradient**\n",
        "- Update rule:  \n",
        "  `Œ∏ = Œ∏ - Œ± * ‚àáL(Œ∏)`  \n",
        "  where:\n",
        "  - `Œ∏` = parameters (weights)\n",
        "  - `Œ±` = learning rate\n",
        "  - `‚àáL(Œ∏)` = gradient of loss\n",
        "\n",
        "**‚úÖ Pros:** Easy to understand  \n",
        "**‚ùå Cons:** Slow, inefficient for large datasets\n",
        "\n",
        "---\n",
        "\n",
        "#### 2. **Stochastic Gradient Descent (SGD)**\n",
        "- Updates weights using **only one data point (or a small batch)** at a time\n",
        "\n",
        "**‚úÖ Pros:** Faster updates, can escape local minima  \n",
        "**‚ùå Cons:** More noisy updates, less stable\n",
        "\n",
        "```python\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "model.compile(optimizer=SGD(learning_rate=0.01), loss='mse')\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "#### 3. **Momentum**\n",
        "- Like SGD but with **inertia** ‚Äî it keeps going in the direction it's already heading\n",
        "- Helps move through ravines in loss landscape faster\n",
        "\n",
        "**‚úÖ Pros:** Faster convergence  \n",
        "**‚ùå Cons:** Needs tuning of momentum term\n",
        "\n",
        "---\n",
        "\n",
        "#### 4. **RMSprop (Root Mean Square Propagation)**\n",
        "- Adjusts the learning rate dynamically for each parameter\n",
        "- Focuses on recent gradients using **moving average**\n",
        "\n",
        "**‚úÖ Pros:** Works well for RNNs and noisy data  \n",
        "**‚ùå Cons:** Can be tricky to tune\n",
        "\n",
        "---\n",
        "\n",
        "#### 5. **Adam (Adaptive Moment Estimation)**\n",
        "- Combines **Momentum + RMSprop**\n",
        "- Most popular optimizer in deep learning\n",
        "- Uses **adaptive learning rates** and **momentum**\n",
        "\n",
        "**‚úÖ Pros:** Works well in most cases, fast, stable  \n",
        "**‚ùå Cons:** May generalize worse than SGD in some cases\n",
        "\n",
        "```python\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy')\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "#### 6. **Adagrad / Adadelta**\n",
        "- Adapts learning rate based on past gradients\n",
        "- Good for sparse data (e.g., text, NLP)\n",
        "\n",
        "---\n",
        "\n",
        "### üîç Summary Table:\n",
        "\n",
        "| Optimizer | Key Feature                    | Best For                     |\n",
        "|-----------|--------------------------------|------------------------------|\n",
        "| GD        | Full batch updates             | Small datasets               |\n",
        "| SGD       | Per-sample updates             | Large, noisy data            |\n",
        "| Momentum  | Adds momentum to updates       | Faster convergence           |\n",
        "| RMSprop   | Adapts learning rate           | RNNs, non-stationary data    |\n",
        "| Adam      | Combines RMSprop + Momentum    | Most deep learning tasks     |\n",
        "| Adagrad   | Adapt learning rate per weight | Sparse data like NLP         |\n",
        "\n",
        "---\n",
        "\n",
        "17.  What is sklearn.linear_model ?\n",
        "- `sklearn.linear_model` is a **module in Scikit-learn** that provides classes for **linear models**, such as:\n",
        "\n",
        "- **Linear Regression**\n",
        "- **Logistic Regression**\n",
        "- **Ridge Regression**\n",
        "- **Lasso Regression**\n",
        "- **ElasticNet**\n",
        "\n",
        "üëâ It‚Äôs used to **build and train models** that assume a **linear relationship** between input features and the target variable.\n",
        "\n",
        "-----\n",
        "18.  What does model.fit() do? What arguments must be given?\n",
        "- Great question! üëá\n",
        "\n",
        "---\n",
        "\n",
        "### üîß **What does `model.fit()` do?**\n",
        "\n",
        "The `.fit()` method is used to **train** a machine learning model.\n",
        "\n",
        "> It **feeds the training data into the model**, so it can **learn the patterns** and relationships between inputs and outputs.\n",
        "\n",
        "---\n",
        "\n",
        "### üìå **Syntax:**\n",
        "```python\n",
        "model.fit(X_train, y_train)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **Required Arguments:**\n",
        "\n",
        "| Argument     | Description                          |\n",
        "|--------------|--------------------------------------|\n",
        "| `X_train`    | Input features (independent variables) |\n",
        "| `y_train`    | Target labels (dependent variable)     |\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Example:\n",
        "```python\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "```\n",
        "\n",
        "After fitting, the model has learned the parameters (like weights) and is ready to make predictions using `.predict()`.\n",
        "\n",
        "---\n",
        "19.  What does model.predict() do? What arguments must be given?\n",
        "- `model.predict()` is used to **make predictions** using a trained model.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **Syntax:**\n",
        "```python\n",
        "model.predict(X_test)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üìå **Required Argument:**\n",
        "- `X_test`: Input features (same format as training data, without labels)\n",
        "\n",
        "---\n",
        "\n",
        "### üîç What it does:\n",
        "It returns the model‚Äôs **predicted output values** (e.g., class labels or numbers) for the given inputs.\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Example:\n",
        "```python\n",
        "predictions = model.predict(X_test)\n",
        "```\n",
        "\n",
        "------\n",
        "20.  What are continuous and categorical variables?\n",
        "- Great question! Let‚Äôs keep it simple üëá\n",
        "\n",
        "---\n",
        "\n",
        "### üìä **Continuous vs. Categorical Variables**\n",
        "\n",
        "#### üî¢ **Continuous Variables:**\n",
        "- **Numerical** values that can take **any value in a range** (can be decimal or fractional).\n",
        "- You can **measure** them.\n",
        "- Infinite possibilities between two values.\n",
        "\n",
        "**Examples:**\n",
        "- Height (e.g., 172.5 cm)\n",
        "- Temperature (e.g., 36.6¬∞C)\n",
        "- Salary (e.g., $45,000.75)\n",
        "\n",
        "---\n",
        "\n",
        "#### üî† **Categorical Variables:**\n",
        "- Values that represent **categories or labels**.\n",
        "- You can **count or classify** them, but not measure.\n",
        "- Often text or codes (but can be numbers used as labels).\n",
        "\n",
        "**Examples:**\n",
        "- Gender (Male, Female)\n",
        "- City (Paris, London, Tokyo)\n",
        "- Blood Type (A, B, AB, O)\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Summary:\n",
        "\n",
        "| Type            | Description                        | Example           |\n",
        "|-----------------|------------------------------------|-------------------|\n",
        "| **Continuous**  | Measurable numbers (range of values) | Height, Age       |\n",
        "| **Categorical** | Labels or categories                | Color, Country    |\n",
        "\n",
        "----\n",
        "21. What is feature scaling? How does it help in Machine Learning?\n",
        "- ### ‚öñÔ∏è **What is Feature Scaling?**\n",
        "\n",
        "Feature scaling is the process of **normalizing or standardizing** the range of independent variables (features) so they are on a **similar scale**.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **Why is it important in Machine Learning?**\n",
        "\n",
        "- üìè Ensures **no feature dominates** others just because of larger values\n",
        "- üöÄ Improves the **performance and convergence speed** of algorithms (especially gradient-based models)\n",
        "- ‚úÖ Essential for models like **KNN, SVM, Logistic Regression, Neural Networks**\n",
        "\n",
        "---\n",
        "\n",
        "### üìå Common Methods:\n",
        "- **Min-Max Scaling:** Scales values to [0, 1]\n",
        "- **Standardization (Z-score):** Scales to mean = 0, std = 1\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Summary:\n",
        "> Feature scaling helps ML models **learn better and faster** by making all features equally important in terms of scale.\n",
        "-----\n",
        "\n",
        "22.  How do we perform scaling in Python?\n",
        "- ### üîß **How to Perform Feature Scaling in Python (short & simple)**\n",
        "\n",
        "Using **Scikit-learn‚Äôs `preprocessing` module**:\n",
        "\n",
        "---\n",
        "\n",
        "### 1. **Standardization (Z-score Scaling)**\n",
        "```python\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Min-Max Scaling (0 to 1)**\n",
        "```python\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "‚úÖ Replace `X` with your feature data (e.g., `X_train`).  \n",
        "Both methods scale your features to improve model performance!\n",
        "\n",
        "------\n",
        "\n",
        "23. What is sklearn.preprocessing?\n",
        "- ### üß∞ **`sklearn.preprocessing` ‚Äì What is it?**\n",
        "\n",
        "`sklearn.preprocessing` is a **module in Scikit-learn** that provides tools to **prepare and transform data** before feeding it into a machine learning model.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **What It‚Äôs Used For:**\n",
        "- **Scaling** features (e.g., `StandardScaler`, `MinMaxScaler`)\n",
        "- **Encoding** categorical variables (e.g., `LabelEncoder`, `OneHotEncoder`)\n",
        "- **Normalizing** data\n",
        "- **Handling missing values**\n",
        "\n",
        "---\n",
        "\n",
        "### üìå Example:\n",
        "```python\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Summary:\n",
        "> `sklearn.preprocessing` helps **clean and prepare your data** so your model can learn from it effectively.\n",
        "\n",
        "---\n",
        "\n",
        "24.  How do we split data for model fitting (training and testing) in Python?\n",
        "- ### ‚úÇÔ∏è **How to Split Data for Training and Testing in Python (Short Answer):**\n",
        "\n",
        "Use `train_test_split` from `sklearn.model_selection`:\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ What It Does:\n",
        "- Splits your data into:\n",
        "  - **Training set** (e.g., 80%)\n",
        "  - **Testing set** (e.g., 20%)\n",
        "- `random_state` ensures the split is **reproducible**.\n",
        "\n",
        "---\n",
        "\n",
        "Now you're ready to **train your model on `X_train, y_train`** and **test it on `X_test, y_test`**!\n",
        "---\n",
        "25. Explain data encoding?\n",
        "- ### üî† **What is Data Encoding?**\n",
        "\n",
        "**Data encoding** is the process of converting **categorical (non-numeric) data** into a **numeric format** so that machine learning models can understand and process it.\n",
        "\n",
        "> Most ML algorithms only work with **numbers**, not text ‚Äî so we encode the text labels!\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **Why is Encoding Important?**\n",
        "- ML models can't handle strings or categories directly\n",
        "- Proper encoding helps models learn patterns from categorical data\n",
        "\n",
        "---\n",
        "\n",
        "### üîß **Common Encoding Techniques:**\n",
        "\n",
        "#### 1. **Label Encoding**\n",
        "- Converts each category to a unique number (e.g., `Red` = 0, `Blue` = 1)\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "encoded = encoder.fit_transform(['Red', 'Blue', 'Green'])\n",
        "```\n",
        "\n",
        "**‚ö†Ô∏è Use only when categories have an order, or for tree-based models.**\n",
        "\n",
        "---\n",
        "\n",
        "#### 2. **One-Hot Encoding**\n",
        "- Creates binary columns for each category (e.g., `Red` ‚Üí `[1, 0, 0]`)\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame({'Color': ['Red', 'Blue', 'Green']})\n",
        "encoded = pd.get_dummies(df, columns=['Color'])\n",
        "```\n",
        "\n",
        "**‚úÖ Best for nominal (no order) categories.**\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Summary:\n",
        "> **Data encoding** translates categories into numbers so ML models can use them.  \n",
        "> Choose **LabelEncoding** for ordered labels, **OneHotEncoding** for unordered ones.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uePrsb0O2ME_"
      }
    }
  ]
}